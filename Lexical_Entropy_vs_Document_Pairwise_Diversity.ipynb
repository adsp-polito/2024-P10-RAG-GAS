{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Lexical Entropy $$H(V_D)$$\n",
    "\n",
    "**Definition.**  \n",
    "Let  \n",
    "$$\n",
    "d = \\{ t_1, \\dots, t_n \\}\n",
    "$$  \n",
    "be a document, and let  \n",
    "$$\n",
    "D = \\{ d_1, \\dots, d_N \\}\n",
    "$$  \n",
    "be a corpus. The vocabulary  \n",
    "$$\n",
    "V_D = \\bigcup_{d \\in D} d\n",
    "$$  \n",
    "is the set of all unique terms in the corpus. For each term $t \\in V_D$, define  \n",
    "$$\n",
    "p_D(t) := \\frac{\\bigl|\\{\\,d \\in D : t \\in d\\}\\bigr|}{|D|}\\,.\n",
    "$$  \n",
    "A term follows a Bernoulli distribution with parameter $p_D(t)$, so its Shannon entropy is  \n",
    "$$\n",
    "H(t) = p_D(t)\\,\\log_2\\!\\Bigl(\\tfrac{1}{p_D(t)}\\Bigr)\\;+\\;(1 - p_D(t))\\,\\log_2\\!\\Bigl(\\tfrac{1}{1 - p_D(t)}\\Bigr).\n",
    "$$  \n",
    "Finally, the **Corpus Lexical Entropy** is  \n",
    "$$\n",
    "H(V_D) := \\sum_{t \\in V_D} H(t).\n",
    "$$\n",
    "\n",
    "**Range.**  \n",
    "- **Minimum**:  \n",
    "  $$\n",
    "  \\min H(V_D) = 0\n",
    "  $$  \n",
    "  occurs if and only if every $p_D(t)\\in\\{0,1\\}$ (i.e.\\ each term is in all documents or in none).  \n",
    "- **Maximum**:  \n",
    "  $$\n",
    "  \\max H(V_D) = |V_D|\n",
    "  $$  \n",
    "  since for each $t$, $H(t)\\le1$, with equality exactly when $p_D(t)=\\tfrac12$.\n",
    "\n",
    "---\n",
    "\n",
    "# Document Pairwise Diversity $$D_J(D)$$\n",
    "\n",
    "**Definition.**  \n",
    "Given the same corpus $$D=\\{d_1,\\dots,d_N\\}$$, let  \n",
    "$$\n",
    "\\delta_J(d_i,d_j) = 1 - \\frac{\\lvert d_i \\cap d_j\\rvert}{\\lvert d_i \\cup d_j\\rvert}\n",
    "$$  \n",
    "be the Jaccard distance between documents. The **average pairwise diversity** is  \n",
    "$$\n",
    "D_J(D) = \\frac{1}{\\binom{N}{2}} \\sum_{1\\le i<j\\le N} \\delta_J(d_i,d_j).\n",
    "$$\n",
    "\n",
    "**Range.**  \n",
    "- **Minimum**:  \n",
    "  $$\n",
    "  \\min D_J(D) = 0\n",
    "  $$  \n",
    "  when all documents share exactly the same set of terms ($\\delta_J=0$ for every pair).  \n",
    "- **Maximum**:  \n",
    "  $$\n",
    "  \\max D_J(D) = 1\n",
    "  $$  \n",
    "  when every pair of documents is disjoint ($\\delta_J=1$ for every pair).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_36108\\2888013456.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "def compute_corpus_lexical_entropy(corpus):\n",
    "    \"\"\"\n",
    "    Computes Corpus Lexical Entropy H(V_D) for a given corpus.\n",
    "    corpus: list of strings (documents)\n",
    "    \"\"\"\n",
    "    docs = [set(doc.lower().split()) for doc in corpus]\n",
    "    N = len(docs)\n",
    "    vocab = set().union(*docs)\n",
    "    \n",
    "    def entropy(p):\n",
    "        if p == 0 or p == 1:\n",
    "            return 0.0\n",
    "        return p * math.log2(1/p) + (1 - p) * math.log2(1/(1 - p))\n",
    "    \n",
    "    H = 0.0\n",
    "    for term in vocab:\n",
    "        p_t = sum(1 for d in docs if term in d) / N\n",
    "        H += entropy(p_t)\n",
    "    return H\n",
    "\n",
    "def compute_average_jaccard(corpus):\n",
    "    \"\"\"\n",
    "    Computes average pairwise Jaccard distance D_J(D) for a given corpus.\n",
    "    corpus: list of strings (documents)\n",
    "    \"\"\"\n",
    "    docs = [set(doc.lower().split()) for doc in corpus]\n",
    "    pairs = list(combinations(docs, 2))\n",
    "    \n",
    "    def jaccard_distance(a, b):\n",
    "        inter = len(a & b)\n",
    "        union = len(a | b)\n",
    "        return 1 - inter / union if union > 0 else 0\n",
    "    \n",
    "    distances = [jaccard_distance(d1, d2) for d1, d2 in pairs]\n",
    "    return sum(distances) / len(distances) if distances else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(corpora):\n",
    "    results = []\n",
    "    for name, corpus in corpora.items():\n",
    "        H = compute_corpus_lexical_entropy(corpus)\n",
    "        DJ = compute_average_jaccard(corpus)\n",
    "        results.append({\n",
    "            \"Corpus\": name,\n",
    "            \"H(V_D)\": round(H, 4),\n",
    "            \"D_J(D)\": round(DJ, 4)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results).set_index(\"Corpus\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H(V_D)</th>\n",
       "      <th>D_J(D)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Corpus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>3.6096</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>4.5915</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>4.6226</td>\n",
       "      <td>0.8611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D4</th>\n",
       "      <td>4.8677</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D5</th>\n",
       "      <td>5.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D6</th>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        H(V_D)  D_J(D)\n",
       "Corpus                \n",
       "D1      3.6096  1.0000\n",
       "D2      4.5915  1.0000\n",
       "D3      4.6226  0.8611\n",
       "D4      4.8677  1.0000\n",
       "D5      5.0000  1.0000\n",
       "D6      5.0000  0.8000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = {\"a\", \"b\", \"c\", \"d\", \"e\"}\n",
    "\n",
    "# D1 just uses one term per document\n",
    "D1 = {\n",
    "    \"a\", \n",
    "    \"b\", \n",
    "    \"c\", \n",
    "    \"d\",\n",
    "    \"e\",\n",
    "} # Expected value of the metric: maximal\n",
    "\n",
    "# D2 has no two documents using the same terms\n",
    "D2 = {\n",
    "    \"a b\",\n",
    "    \"c\", \n",
    "    \"d e\",\n",
    "} # Expected value of the metric: maximal\n",
    "\n",
    "#D3 adds to D2 a new document, adding no new terms\n",
    "D3 = {\n",
    "    \"a b\", \n",
    "    \"c\", \n",
    "    \"d e\",\n",
    "    \"b c e\", # we expect a lower diversity than D2, because it's repeating terms.\n",
    "}\n",
    "\n",
    "# D4 adds to D2 a new document, with a new term\n",
    "D4 = {\n",
    "    \"a b\",\n",
    "    \"c\", \n",
    "    \"d e\", \n",
    "    \"f\",\n",
    "}\n",
    "\n",
    "# D5 splits the vocabulary in two parts, reaching the theoretical maximum for H(Vd)\n",
    "D5 = {\n",
    "    \"a d e\",\n",
    "    \"b c\"\n",
    "}\n",
    "\n",
    "\n",
    "# In D6, each term {a, b, c, d, e} appears in exactly half of the documents.\n",
    "D6 = {\n",
    "    \"a b\",   \n",
    "    \"a c e\",  \n",
    "    \"b d e\", \n",
    "    \"c d\"    \n",
    "} \n",
    "\n",
    "run_experiment({'D1' : D1, \"D2\" : D2, \"D3\": D3, \"D4\" : D4, \"D5\": D5, \"D6\": D6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we mean for \"diversity\"? To me, diversity in this context should be more *document-oriented*: Given a corpus D, picking two random documents, the less terms in common they have, the higher the diversity.\n",
    "\n",
    "$H(V_D)$ focuses more on terms distribution within the corpus. Which can be ok, but may not be optimal for our goal.\n",
    "\n",
    "At this stage, I would rather prefer H(D):\n",
    "- In both D1 and D2, we can pick any pair of documents, we will find no intersection between them. Therefore, the diversity should be maximal for both ->  **$D_j(D)$ does, $H(V_D)$ does not.**\n",
    "- D1 and D5 should be both maximal, because documents are totally diverse (as in D1 vs D2) ->  **$D_j(D)$ does, $H(V_D)$ does not.** ;\n",
    "- In D6, if you pick the first document and last document, then they are totally diverse. Otherwise, no matter the pair you take, there will be some intersection between documents. This should be reflected by a lower level of divesity ->  **$D_j(D)$ does, $H(V_D)$ does not.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
